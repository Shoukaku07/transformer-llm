# transformer-llm

A minimal implementation of a Transformer-based Language Model designed for learning and experimentation.

## Purpose

This project provides a simple implementation of the Transformer language model (Causal LM) with the goals of:

* Understanding the core mechanisms of the Transformer architecture.
* Serving as a basis for rapid experimentation and modification.

## Architecture

The model implements essential components of a Transformer decoder-based language model, including:

* Token embedding
* Rotary embeddings
* Self-attention
* Feed-forward
* RMS Normalization

---

## License

This project is licensed under the [GPL v3 License](LICENSE) - see the LICENSE file for details.

**Note:** The tokenizer used in this project is based on GPT-2 and is licensed under the [MIT License](tokenizer/LICENSE). Please refer to the tokenizer/LICENSE file for the tokenizerâ€™s license information.
